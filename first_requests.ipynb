{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# openAI chatGPT first requests\n",
    "Based on ideas from https://www.udemy.com/course/mastering-openai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/john/anaconda3/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (2.0.4) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import join, dirname\n",
    "import openai\n",
    "import dotenv\n",
    "from dotenv import dotenv_values, load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv_path = join('..', '.env')\n",
    "load_dotenv(dotenv_path)\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "#config = dotenv_values(\".env\")\n",
    "#openai.api_key = config[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completion Examples\n",
    "see https://platform.openai.com/docs/api-reference/completions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7iNtCc2c6nawXFSPTqlBz47gyQt6M at 0x7f1146e42a70> JSON: {\n",
       "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
       "  \"id\": \"cmpl-7iNtCc2c6nawXFSPTqlBz47gyQt6M\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1690812618,\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \"\\nMeow!\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"stop\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 4,\n",
       "    \"completion_tokens\": 4,\n",
       "    \"total_tokens\": 8\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"The cat says \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
      "  \"id\": \"cmpl-7hJ4ErHFIpjgbeoKgTAlPI44vUrdN\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1690555754,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\n\\n1. The Godfather (1972) \\n2. The Shawshank Redemption (1994)\\n3. Raging Bull (1980)\\n4. Casablanca (1942)\\n5. Citizen Kane (1941)\\n\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 12,\n",
      "    \"completion_tokens\": 51,\n",
      "    \"total_tokens\": 63\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "1. The Godfather (1972) \n",
      "2. The Shawshank Redemption (1994)\n",
      "3. Raging Bull (1980)\n",
      "4. Casablanca (1942)\n",
      "5. Citizen Kane (1941)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Create a list of the best movies of all time: \",\n",
    "    max_tokens=200, #increase max_tokens\n",
    "    stop=\"6.\" #stop at text 6. (anything after and including 6. will not be returned)\n",
    ")\n",
    "print(response)\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That sounds like fun! What songs do you like to sing?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "You are a chatbot speaking like a toddler. \n",
    "\n",
    "User: Hi, how are you?\n",
    "Chatbot: I'm good. \n",
    "User: Tell me about your family.\n",
    "Chatbot: I have a mommy and a daddy and baby sister and two kitties. \n",
    "User: What do you do for fun?\n",
    "Chatbot: I like to play make-believe with my baby sister. I like to pretend to be doggies and kitties and I like to sing and dance too.\n",
    "User:\n",
    "\n",
    "\"\"\"\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=prompt,\n",
    "    max_tokens=200, #increase max_tokens\n",
    "    stop=[\"Chatbot:\", \"User:\"] # stop after one response from either user or chatbot\n",
    ")\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
      "  \"id\": \"cmpl-7hL3ge0RoqyE5Yhar5qnS4wZWgdBj\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1690563408,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"Make a joke about chickens \\n\\nQ: Why don't chickens like people? \\nA: Because they always get picked on!\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Make a joke about chickens \\nQ: Why did the chicken go to the s\\u00e9ance? \\nA: To get to the other side!\",\n",
      "      \"index\": 1,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Make a joke about chickens \\n\\nQ: Why did the chicken go to the s\\u00e9ance? \\nA: To get to the other side!\",\n",
      "      \"index\": 2,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Make a joke about chickens \\n\\nQ: What did the chicken say when it crossed the playground? \\nA: \\\"Hey everyone, I'm going to be some one's dinner tonight!\\\"\",\n",
      "      \"index\": 3,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    },\n",
      "    {\n",
      "      \"text\": \"Make a joke about chickens \\n\\nQ: What did the chicken say when it crossed the road?\\nA: \\\"Cluck, cluck, get out of my way!\\\"\",\n",
      "      \"index\": 4,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 6,\n",
      "    \"completion_tokens\": 138,\n",
      "    \"total_tokens\": 144\n",
      "  }\n",
      "}\n",
      "###########\n",
      "Make a joke about chickens \n",
      "\n",
      "Q: Why don't chickens like people? \n",
      "A: Because they always get picked on!\n",
      "Make a joke about chickens \n",
      "Q: Why did the chicken go to the s√©ance? \n",
      "A: To get to the other side!\n",
      "Make a joke about chickens \n",
      "\n",
      "Q: Why did the chicken go to the s√©ance? \n",
      "A: To get to the other side!\n",
      "Make a joke about chickens \n",
      "\n",
      "Q: What did the chicken say when it crossed the playground? \n",
      "A: \"Hey everyone, I'm going to be some one's dinner tonight!\"\n",
      "Make a joke about chickens \n",
      "\n",
      "Q: What did the chicken say when it crossed the road?\n",
      "A: \"Cluck, cluck, get out of my way!\"\n"
     ]
    }
   ],
   "source": [
    "#changing number of responses, n\n",
    "#add echo (not billed extra for echo)\n",
    "n=5\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Make a joke about chickens \",\n",
    "    max_tokens=200, #increase max_tokens\n",
    "    n=n,\n",
    "    echo=True\n",
    ")\n",
    "print(response)\n",
    "print(\"###########\")\n",
    "for i in range(0, n):\n",
    "    print(response[\"choices\"][i][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
      "  \"id\": \"cmpl-7heLv5Vp3XPZ78WUVVzuNwTMtwpLP\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1690637575,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\n\\ntop_movies: The Godfather, The Shawshank Redemption, Casablanca, The Godfather Part II, The Dark Knight, Schindler's List, Pulp Fiction, 12 Angry Men, The Lord of the Rings: The Return of the King, The Good, the Bad and the Ugly, Star Wars, Seven Samurai, Forrest Gump, Inception, Fight Club, The Matrix, Goodfellas\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 31,\n",
      "    \"completion_tokens\": 89,\n",
      "    \"total_tokens\": 120\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "top_movies: The Godfather, The Shawshank Redemption, Casablanca, The Godfather Part II, The Dark Knight, Schindler's List, Pulp Fiction, 12 Angry Men, The Lord of the Rings: The Return of the King, The Good, the Bad and the Ugly, Star Wars, Seven Samurai, Forrest Gump, Inception, Fight Club, The Matrix, Goodfellas\n"
     ]
    }
   ],
   "source": [
    "#prompt engineering, CSV output\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Create a list of the best movies of all time.  Desired format: top_movies: <comma_separated_list> \",\n",
    "    max_tokens=200, #increase max_tokens\n",
    ")\n",
    "print(response)\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
      "  \"id\": \"cmpl-7heQDDyE7DWhP5GasIZ72AHeuGnmt\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1690637841,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\n\\npython_dict = {\\n    'China': 1433783686,\\n    'India': 1366417754,\\n    'United States': 331002651,\\n    'Indonesia': 269603400,\\n    'Brazil': 211049527\\n}\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 26,\n",
      "    \"completion_tokens\": 62,\n",
      "    \"total_tokens\": 88\n",
      "  }\n",
      "}\n",
      "##########\n",
      "\n",
      "\n",
      "python_dict = {\n",
      "    'China': 1433783686,\n",
      "    'India': 1366417754,\n",
      "    'United States': 331002651,\n",
      "    'Indonesia': 269603400,\n",
      "    'Brazil': 211049527\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#prompt engineering, create python dictionary\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Create a list of the top five most populous countries and earth with their population.  Desired output: <python_dict>\",\n",
    "    max_tokens=200, #increase max_tokens\n",
    ")\n",
    "print(response)\n",
    "print(\"##########\")\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
      "  \"id\": \"cmpl-7heSTtdhBUXy7uOrMknfJRPPP2Opq\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1690637981,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \".\\n\\n{\\n    \\\"China\\\":  1409517397,\\n    \\\"India\\\": 1366417754,\\n    \\\"United States\\\": 331002651,\\n    \\\"Indonesia\\\": 268663640,\\n    \\\"Brazil\\\": 212559417\\n  }\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 33,\n",
      "    \"completion_tokens\": 60,\n",
      "    \"total_tokens\": 93\n",
      "  }\n",
      "}\n",
      "##########\n",
      ".\n",
      "\n",
      "{\n",
      "    \"China\":  1409517397,\n",
      "    \"India\": 1366417754,\n",
      "    \"United States\": 331002651,\n",
      "    \"Indonesia\": 268663640,\n",
      "    \"Brazil\": 212559417\n",
      "  }\n"
     ]
    }
   ],
   "source": [
    "#prompt engineering, JSON object\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Create a list of the top five most populous countries on earth with their population.  Desired output: JSON object with name as the key and population as the value\",\n",
    "    max_tokens=200, #increase max_tokens\n",
    ")\n",
    "print(response)\n",
    "print(\"##########\")\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping with Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/john/anaconda3/lib/python3.7/site-packages (2.22.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/john/anaconda3/lib/python3.7/site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/john/anaconda3/lib/python3.7/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/john/anaconda3/lib/python3.7/site-packages (from requests) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/john/anaconda3/lib/python3.7/site-packages (from requests) (1.25.8)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/john/anaconda3/lib/python3.7/site-packages (4.8.2)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /home/john/anaconda3/lib/python3.7/site-packages (from beautifulsoup4) (1.9.5)\n",
      "Requirement already satisfied: urllib3 in /home/john/anaconda3/lib/python3.7/site-packages (1.25.8)\n",
      "Requirement already satisfied: nltk in /home/john/anaconda3/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /home/john/anaconda3/lib/python3.7/site-packages (from nltk) (1.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/john/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/john/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
      "  \"id\": \"cmpl-7hwZhSMx4SKm89jKnz2UNaPmAnAiJ\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1690707621,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\n\\n Marcus Tullius Cicero (January BC - December BC) was a Roman statesman, lawyer, scholar, philosopher, writer and academic skeptic. He created a Latin philosophical vocabulary with neologisms and was considered one of Rome's greatest orators and prose stylists. He suppressed the Catiline conspiracy, championed a return to traditional republican government and was proscribed as an enemy of the state by the Second Triumvirate. His works are influential in global culture and his rediscovery in the 14th century began the Renaissance.\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 759,\n",
      "    \"completion_tokens\": 111,\n",
      "    \"total_tokens\": 870\n",
      "  }\n",
      "}\n",
      "##########\n",
      "\n",
      "\n",
      " Marcus Tullius Cicero (January BC - December BC) was a Roman statesman, lawyer, scholar, philosopher, writer and academic skeptic. He created a Latin philosophical vocabulary with neologisms and was considered one of Rome's greatest orators and prose stylists. He suppressed the Catiline conspiracy, championed a return to traditional republican government and was proscribed as an enemy of the state by the Second Triumvirate. His works are influential in global culture and his rediscovery in the 14th century began the Renaissance.\n"
     ]
    }
   ],
   "source": [
    "#Wikipedia Summary\n",
    "\n",
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install urllib3\n",
    "!pip install nltk\n",
    "import urllib\n",
    "import requests\n",
    "import bs4 as bs\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "source = urllib.request.urlopen('https://en.wikipedia.org/wiki/Cicero').read()\n",
    "soup = bs.BeautifulSoup(source,'lxml')\n",
    "text = \"\"\n",
    "for paragraph in soup.find_all('p'):\n",
    "    text += paragraph.text\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\d',' ',text)\n",
    "text = re.sub(r'\"', '', text)\n",
    "text = re.sub(\"'\", '', text)\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "openai_prompt=\"Summarize the following text:\"\n",
    "for i in range(20):\n",
    "    openai_prompt+=sentences[i]\n",
    "    \n",
    "#prompt scraped from wikipedia\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=openai_prompt,\n",
    "    max_tokens=200, #increase max_tokens\n",
    ")\n",
    "print(response)\n",
    "print(\"##########\")\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /home/john/anaconda3/lib/python3.7/site-packages (2.22.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/john/anaconda3/lib/python3.7/site-packages (from requests) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/john/anaconda3/lib/python3.7/site-packages (from requests) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/john/anaconda3/lib/python3.7/site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/john/anaconda3/lib/python3.7/site-packages (from requests) (1.25.8)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/john/anaconda3/lib/python3.7/site-packages (4.8.2)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /home/john/anaconda3/lib/python3.7/site-packages (from beautifulsoup4) (1.9.5)\n",
      "Requirement already satisfied: urllib3 in /home/john/anaconda3/lib/python3.7/site-packages (1.25.8)\n",
      "Requirement already satisfied: nltk in /home/john/anaconda3/lib/python3.7/site-packages (3.4.5)\n",
      "Requirement already satisfied: six in /home/john/anaconda3/lib/python3.7/site-packages (from nltk) (1.14.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/john/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/john/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########\n",
      "\n",
      "\n",
      "{\n",
      "  \"flower power\": [\"India Pale Ale\", \"Simcoe, Centennial, Cascade, Ahtanum\", \"6.5%\"],\n",
      "  \"everyday haze\": [\"India Pale Ale\", \"Citra, Mosaic, Lotos\", \"6.9%\"],\n",
      "  \"apricot wheat\": [\"Wheat Ale\", \"Summit\", \"4.3%\"],\n",
      "  \"lakeside lager\": [\"Lager\", \"Hallertau, Mittlefruh, Lublin\", \"5.2%\"],\n",
      "  \"nut brown ale\": [\"Brown Ale\", \"Northern Brewer\", \"4.7%\"],\n",
      "  \"cascazilla\": [\"Red India Pale Ale\", \"Crystal, Chinook, Cascade, Amarillo\", \"6.9%\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Website Menu Items to JSON\n",
    "\n",
    "!pip install requests\n",
    "!pip install beautifulsoup4\n",
    "!pip install urllib3\n",
    "!pip install nltk\n",
    "import urllib\n",
    "import requests\n",
    "import bs4 as bs\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "url = \"https://ithacabeer.com/ithaca-beer-core-beliefs\"\n",
    "req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "webpage = urllib.request.urlopen(req).read()\n",
    "soup = bs.BeautifulSoup(webpage,'lxml')\n",
    "text = \"\"\n",
    "for paragraph in soup.find_all('p'):\n",
    "    text += paragraph.text\n",
    "text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "text = re.sub(r'\\s+',' ',text)\n",
    "text = text.lower()\n",
    "text = re.sub(r'\\d',' ',text)\n",
    "text = re.sub(r'\"', '', text)\n",
    "text = re.sub(\"'\", '', text)\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "openai_prompt=\"Extract drinks along with type, hops and ABV from the following text. Desired output: JSON object with food name as the key and type, hops, ABV as values. \"\n",
    "for i in range(len(sentences)):\n",
    "    openai_prompt+=sentences[i]\n",
    "   \n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=openai_prompt,\n",
    "    max_tokens=200, #increase max_tokens\n",
    ")\n",
    "#print(response)\n",
    "print(\"##########\")\n",
    "print(response[\"choices\"][0][\"text\"])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot Prompting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
      "  \"id\": \"cmpl-7hxXSAjL5qnfM4JPQVSz1e3NmBUtg\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1690711326,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \" Sentiment: 0\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 72,\n",
      "    \"completion_tokens\": 4,\n",
      "    \"total_tokens\": 76\n",
      "  }\n",
      "}\n",
      " Sentiment: 0\n"
     ]
    }
   ],
   "source": [
    "openai_prompt = \"Classify the following text as negative, neutral, or positive.  \\\n",
    "            Desired Format: -1 negative, 0 neutral, 1 positive \\\n",
    "            Input: Love the Food! \\\n",
    "            Sentiment: 1 \\\n",
    "            Input: Worst food since food went to food town \\\n",
    "            Sentiment: -1 \\\n",
    "            The ambiance was nice, but the food was so-so.  \"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=openai_prompt,\n",
    "    max_tokens=200, #increase max_tokens\n",
    ")\n",
    "print(response)\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain of Thought Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
      "  \"id\": \"cmpl-7hxh2f5dsAuBxxloUZOHwE8hK8sr3\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1690711920,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\n\\nAlice is 7 years older than Beth, so Alice is 7 years older than Erica when Erica is 30 years old, since Beth is 5 years older than Erica. Therefore, the difference between the ages of Alice and Erica is 7 years.\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 46,\n",
      "    \"completion_tokens\": 49,\n",
      "    \"total_tokens\": 95\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Alice is 7 years older than Beth, so Alice is 7 years older than Erica when Erica is 30 years old, since Beth is 5 years older than Erica. Therefore, the difference between the ages of Alice and Erica is 7 years.\n"
     ]
    }
   ],
   "source": [
    "openai_prompt = \"Alice is 7 years older than Beth, \\\n",
    "                who is 5 years older than Erica. \\\n",
    "                What is the difference between the ages of Alice and Erica, \\\n",
    "                if Erica is 30 years old? \\\n",
    "                Let's think step by step:\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=openai_prompt,\n",
    "    max_tokens=200, #increase max_tokens\n",
    ")\n",
    "print(response)\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
      "  \"id\": \"cmpl-7hxmlC2Gwgy8V7EWq9P0s9Kk8n7Gz\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1690712275,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\n\\nSpanish: Inserte el texto aqu\\u00ed.\\nPolish: Wstaw tekst tutaj.\\nLatin: Textum hic inserat.\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 20,\n",
      "    \"completion_tokens\": 37,\n",
      "    \"total_tokens\": 57\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Spanish: Inserte el texto aqu√≠.\n",
      "Polish: Wstaw tekst tutaj.\n",
      "Latin: Textum hic inserat.\n"
     ]
    }
   ],
   "source": [
    "openai_prompt = \"Translate the following text to Spanish, Polish, and Latin \\\n",
    "                Text: Insert text here. \"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=openai_prompt,\n",
    "    max_tokens=200, #increase max_tokens\n",
    ")\n",
    "print(response)\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
      "  \"id\": \"cmpl-7hxpTuNAJ5c2rSvjI1prXa9NwgOi1\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1690712443,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\n\\nShe will love to ski and go to the discotech.\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 29,\n",
      "    \"completion_tokens\": 14,\n",
      "    \"total_tokens\": 43\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "She will love to ski and go to the discotech.\n"
     ]
    }
   ],
   "source": [
    "openai_prompt = \"Transform the following text from 1st to 3rd person in future tense\\\n",
    "                Text: I love to ski and go to the discotech. \"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=openai_prompt,\n",
    "    max_tokens=200, #increase max_tokens\n",
    ")\n",
    "print(response)\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
      "  \"id\": \"cmpl-7hxsg4CJ0ZLJFgmfW7gjUvg8Cnpxf\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1690712642,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\n\\n1. The Godfather \\ud83d\\ude07\\n2. The Shawshank Redemption \\ud83d\\ude4c\\n3. Schindler's List \\ud83d\\udc4f\\n4. Casablanca \\ud83c\\udfac\\n5. Raging Bull \\ud83d\\udc02\\n6. Citizen Kane \\ud83d\\udd70\\n7. Sunset Boulevard \\ud83c\\udf05\\n8. The Wizard of Oz \\ud83c\\udf08\\n9. Apocalypse Now \\ud83d\\udca5\\n10. The Lord of the Rings \\ud83d\\udde1\\ufe0f\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 28,\n",
      "    \"completion_tokens\": 94,\n",
      "    \"total_tokens\": 122\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "1. The Godfather üòá\n",
      "2. The Shawshank Redemption üôå\n",
      "3. Schindler's List üëè\n",
      "4. Casablanca üé¨\n",
      "5. Raging Bull üêÇ\n",
      "6. Citizen Kane üï∞\n",
      "7. Sunset Boulevard üåÖ\n",
      "8. The Wizard of Oz üåà\n",
      "9. Apocalypse Now üí•\n",
      "10. The Lord of the Rings üó°Ô∏è\n"
     ]
    }
   ],
   "source": [
    "openai_prompt = \"Create a list of the best movies of all time.\\\n",
    "                Desired output:  Movie title, transform movie title to emojis.\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=openai_prompt,\n",
    "    max_tokens=200, #increase max_tokens\n",
    ")\n",
    "print(response)\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
      "  \"id\": \"cmpl-7iNOQyT9i7vBmRyYhdUgXy6sZlpSz\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1690810710,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"\\n\\n\\\\Lorem\\\\ ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 162,\n",
      "    \"completion_tokens\": 157,\n",
      "    \"total_tokens\": 319\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "\\Lorem\\ ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n"
     ]
    }
   ],
   "source": [
    "openai_prompt = \"Transform the following text into latex format: \\\n",
    "                Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=openai_prompt,\n",
    "    max_tokens=200, #increase max_tokens\n",
    ")\n",
    "print(response)\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
      "  \"id\": \"cmpl-7iNYD7XLS5O1mUYWPfLj0sU1zHvrM\",\n",
      "  \"object\": \"text_completion\",\n",
      "  \"created\": 1690811317,\n",
      "  \"model\": \"text-davinci-003\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"text\": \"                                                          \\n(defun maxPalindrome (A)\\n  (let ((s1 A)\\n\\t(s2 '()))\\n    (setq numpy:table (make-array (list (length A) (length A)) :initial-element 0))\\n    (dolist (i (reverse A))\\n      (push i s2))\\n    ;fill in the top and left rows first \\n    (dotimes (i (length s1))\\n      (if (char-equal (nth i s1) (nth 0 s2))\\n\\t  (setf (numpy:element i 0 numpy:table) 1)))\\n    (dotimes (i (length s1))\\n      (if (char-equal (nth 0 s1) (nth i s2))\\n\\t  (setf (numpy:element 0 i numpy:table) 1)))\\n    ;fill in the table \\n    (dotimes (i (1- (length s1)))\\n      (dotimes (j (1- (length s2)))\\n\\t(if (char-equal (nth i s1) (nth j s2))\\n\\t    (setf (numpy:element i j numpy:table) (+  (numpy:element (1- i) (1- j) numpy:table) 1)))))\\n    (numpy:max numpy:table)))\",\n",
      "      \"index\": 0,\n",
      "      \"logprobs\": null,\n",
      "      \"finish_reason\": \"stop\"\n",
      "    }\n",
      "  ],\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 260,\n",
      "    \"completion_tokens\": 316,\n",
      "    \"total_tokens\": 576\n",
      "  }\n",
      "}\n",
      "                                                          \n",
      "(defun maxPalindrome (A)\n",
      "  (let ((s1 A)\n",
      "\t(s2 '()))\n",
      "    (setq numpy:table (make-array (list (length A) (length A)) :initial-element 0))\n",
      "    (dolist (i (reverse A))\n",
      "      (push i s2))\n",
      "    ;fill in the top and left rows first \n",
      "    (dotimes (i (length s1))\n",
      "      (if (char-equal (nth i s1) (nth 0 s2))\n",
      "\t  (setf (numpy:element i 0 numpy:table) 1)))\n",
      "    (dotimes (i (length s1))\n",
      "      (if (char-equal (nth 0 s1) (nth i s2))\n",
      "\t  (setf (numpy:element 0 i numpy:table) 1)))\n",
      "    ;fill in the table \n",
      "    (dotimes (i (1- (length s1)))\n",
      "      (dotimes (j (1- (length s2)))\n",
      "\t(if (char-equal (nth i s1) (nth j s2))\n",
      "\t    (setf (numpy:element i j numpy:table) (+  (numpy:element (1- i) (1- j) numpy:table) 1)))))\n",
      "    (numpy:max numpy:table)))\n"
     ]
    }
   ],
   "source": [
    "openai_prompt = \"Convert the following python code to lisp \\\n",
    "                \\\n",
    "                #bottom-up DP strategy for finding a max palindrome \\\n",
    "                import numpy \\\n",
    "                \\\n",
    "                def maxPalindrome(A): \\\n",
    "                s1 = A \\\n",
    "                s2 = [] \\\n",
    "                numpy.table = numpy.zeros((len(A),len(A))) \\\n",
    "                \\\n",
    "                for i in reversed(A): \\\n",
    "                    s2.append(i) \\\n",
    "                \\\n",
    "                #fill in the top and left rows first \\\n",
    "                for i in range(0, len(s1)): \\\n",
    "                    if s1[i]==s2[0]: \\\n",
    "                        numpy.table[i][0]=1 \\\n",
    "                \\\n",
    "                for i in range(0, len(s1)): \\\n",
    "                    if s1[0]==s2[i]: \\\n",
    "                        numpy.table[0][i]=1 \\\n",
    "                \\\n",
    "                #fill in the table \\\n",
    "                for i in range(1,len(s1)): \\\n",
    "                    for j in range(1, len(s2)): \\\n",
    "                        if s1[i]==s2[j]: \\\n",
    "                            numpy.table[i][j]=1+numpy.table[i-1][j-1] \\\n",
    "                \\\n",
    "                return numpy.max(numpy.table)\"\n",
    "\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=openai_prompt,\n",
    "    max_tokens=500, #increase max_tokens\n",
    ")\n",
    "print(response)\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temperature\n",
    "Temperature changes the softmax scaling distribution for next tokens.  Higher temperatures (0<=t<=2) provide a less scaled distribution than lower temperatures for the softmax selection function, thereby balancing low-valued words and injecting a higher degree of randomness into responses.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7k888bVLCh3pqrCnh5NjBtZgp4Sme at 0x7f427ca86290> JSON: {\n",
       "  \"warning\": \"This model version is deprecated. Migrate before January 4, 2024 to avoid disruption of service. Learn more https://platform.openai.com/docs/deprecations\",\n",
       "  \"id\": \"cmpl-7k888bVLCh3pqrCnh5NjBtZgp4Sme\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"created\": 1691228696,\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"text\": \"\\n\\nOnce upon a time; there was a rancher looking after the family wild amount of tenores roam family food kitchen cover wondering the same area looking cowboy meaderito pup some branch felt visiting for eighteen hun nothing sykov lo largen could monizment cubbing creek charge jullat ventaily christ snow begin traverse cobrabiding goal roads state enganian browvin from mid territory opening seeing clear ratizes finally had campasy plans tried smallening casondirl convoy crosscountry climbing grazters adventure seemed koons popping eagle flylo put cloud briffing squalls. Walking northranch cowboy wil look feels staying twilightc mac roro twenty really fascutilial herd gazing aimeds tinangs certib scamp broken glunkship sung straightdown kickdown lasting reached tough made servoir cirkiirt sod seatcing morning lot blessed rivers tap down tortuable fel roaming search prize glow iron beasts been funny licland fallentreatiny tropogs swootied mass segent hills trave\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"finish_reason\": \"length\"\n",
       "    }\n",
       "  ],\n",
       "  \"usage\": {\n",
       "    \"prompt_tokens\": 5,\n",
       "    \"completion_tokens\": 200,\n",
       "    \"total_tokens\": 205\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"Tell me a story \",\n",
    "    max_tokens=200, \n",
    "    temperature=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nice_print(dictionary):\n",
    "    for key, value in dictionary.items():\n",
    "        wrapped_text = \"\\n\".join(textwrap.wrap(value, width=120))\n",
    "        print(f\"{key}:\")\n",
    "        print(wrapped_text)\n",
    "        print(\"=\" * 120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0:\n",
      "My favorite food is pizza. I love the combination of the doughy crust, the tangy tomato sauce, and the melty cheese. I\n",
      "also like to add different toppings like pepperoni, mushrooms, and olives. Pizza is a great meal for any occasion\n",
      "========================================================================================================================\n",
      "Temperature 1:\n",
      "My favorite food is Chinese food. I love the combination of sweet, savory, and spicy flavors in many Chinese dishes.\n",
      "From fried rice, to hot and sour soup, to orange chicken, Chinese food is sure to satisfy any craving. Plus, there are\n",
      "plenty\n",
      "========================================================================================================================\n",
      "Temperature 2:\n",
      "My favorite food is pizza among finding many flavours must trying corn savarl Tamidity-Remo songs from the jungle Ek\n",
      "barelapenchafulya with lag nav material typically type `geroi janapo fin and spun flakes bayaitasugrulyali\"\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "nice_print(\n",
    "    {\n",
    "        f\"Temperature {temperature}\": openai.Completion.create(model=\"text-davinci-003\", prompt=\"\"\"My favorite food is\"\"\".strip(), max_tokens=50, echo=True, temperature=temperature)\n",
    "        .choices[0][\"text\"]\n",
    "        .strip()\n",
    "        for temperature in [0, 1, 2]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top p\n",
    "top p (0<=top_p<=1) thresholds the top percent in next token distribution creating a selection from fewer tokens. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top P 1:\n",
      "My favorite food is pizza. Pizza is a classic comfort food and I love its combination of melted cheese and chewy crust.\n",
      "It is also incredibly versatile - you can top it with almost any ingredient. I enjoy creating custom pizzas with all of\n",
      "my favorite toppings, like fresh vegetables, pepperoni, and olives. Pizza is also great as an appetizer, snack, or even\n",
      "breakfast\n",
      "========================================================================================================================\n",
      "Top P 0.5:\n",
      "My favorite food is pizza. I love the combination of the doughy crust, the melted cheese, and the variety of toppings\n",
      "that can be added. I also love the convenience of being able to order a pizza for delivery or pick it up at a local\n",
      "pizzeria. Pizza is a great meal for any occasion, whether it's a family gathering, a night out with friends, or just\n",
      "========================================================================================================================\n",
      "Top P 0:\n",
      "My favorite food is pizza. I love the combination of the doughy crust, the tangy tomato sauce, and the melty cheese. I\n",
      "also like to add different toppings like pepperoni, mushrooms, and olives. Pizza is a great meal for any occasion,\n",
      "whether it's a casual night in or a special celebration.\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "nice_print(\n",
    "    {\n",
    "        f\"Top P {top_p}\": openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=\"\"\"My favorite food is\"\"\".strip(),\n",
    "            max_tokens=75,\n",
    "            echo=True,\n",
    "            top_p=top_p,\n",
    "        )\n",
    "        .choices[0][\"text\"]\n",
    "        .strip()\n",
    "        for top_p in [1, 0.5, 0]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency Penalty\n",
    "Higher values (>0) penalize and lower values encourage (<0) new tokens based on existing frequency in the text (-2<=frequency_penalty<=2, default=0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Penalty -2:\n",
      "The first 15 elements are Hydrogen, Helium, Lithium, Beryllium, Boron, Carbon, Nitrogen, Oxygen, Fluorine, Neon,,,,,,,,,\n",
      ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,\n",
      "========================================================================================================================\n",
      "Frequency Penalty 0:\n",
      "The first 15 elements are Hydrogen, Helium, Lithium, Beryllium, Boron, Carbon, Nitrogen, Oxygen, Fluorine, Neon, Sodium,\n",
      "Magnesium, Aluminum, Silicon, Phosphorus.\n",
      "========================================================================================================================\n",
      "Frequency Penalty 2:\n",
      "The first 15 elements are Hydrogen, Helium, Lithium, Beryllium, Boron, Carbon, Nitrogen, Oxygen , Fluorine , Neon ,\n",
      "Sodium  , Magnesium  , Aluminum  Silicon   Phosphorus.\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "nice_print(\n",
    "    {\n",
    "        f\"Frequency Penalty {x}\": openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=\"\"\"The first 15 elements are Hydrogen, Helium,\"\"\".strip(),\n",
    "            max_tokens=200,\n",
    "            echo=True,\n",
    "            frequency_penalty=x,\n",
    "        )\n",
    "        .choices[0][\"text\"]\n",
    "        .strip()\n",
    "        for x in [-2, 0, 2]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presence Penalty\n",
    "Similar to frequency penalty except non-cumulative (higher frequency does not increase the penalty).  Can be used to increase the models likelihood to discuss new topics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Presence Penalty -2:\n",
      "The first 15 elements are Hydrogen, Helium, Lithium, Beryllium, Boron, Carbon, Nitrogen, Oxygen, Fluorine, Neon, Sodium,\n",
      "Magnesium, Aluminum, Silicon, and Phosphorus.\n",
      "========================================================================================================================\n",
      "Presence Penalty 0:\n",
      "The first 15 elements are Hydrogen, Helium, Lithium, Beryllium, Boron, Carbon, Nitrogen, Oxygen, Fluorine, Neon, Sodium,\n",
      "Magnesium, Aluminium, Silicon, Phosphorus.\n",
      "========================================================================================================================\n",
      "Presence Penalty 2:\n",
      "The first 15 elements are Hydrogen, Helium, Lithium, Beryllium, Boron, Carbon, Nitrogen, Oxygen, Fluorine, Neon, Sodium,\n",
      "Magnesium, Aluminum, Silicon, and Phosphorus.\n",
      "========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "nice_print(\n",
    "    {\n",
    "        f\"Presence Penalty {x}\": openai.Completion.create(\n",
    "            model=\"text-davinci-003\",\n",
    "            prompt=\"\"\"The first 15 elements are Hydrogen, Helium,\"\"\".strip(),\n",
    "            max_tokens=200,\n",
    "            echo=True,\n",
    "            presence_penalty=x,\n",
    "        )\n",
    "        .choices[0][\"text\"]\n",
    "        .strip()\n",
    "        for x in [-2, 0, 2]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream\n",
    "Send data as it's being generated instead of waiting for a complete response.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "A foggy morn's a fast commute\n",
      "To the city from the distant villages\n",
      "The dusky landscape far away\n",
      "Asight engulfed in morning haze\n",
      "\n",
      "As night gives way to day\n",
      "The cars and buses make their way\n",
      "Adorned with morning fog and mist\n",
      "Passing by in endless lists\n",
      "\n",
      "The world at its quietest\n",
      "A low hum, the drum beats\n",
      "Towards chrome-lined buildings we hasten\n",
      "Eager to head for our destination\n",
      "\n",
      "The mist rolling in from either side\n",
      "The paths revealed with a squinted eye\n",
      "The night creeps along at a snail‚Äôs pace\n",
      "Till the day dawns over this urban space."
     ]
    }
   ],
   "source": [
    "for data in openai.Completion.create(\n",
    "    model=\"text-davinci-003\",\n",
    "    prompt=\"write me a poem about foggy morning commutes\",\n",
    "    max_tokens=300,\n",
    "    stream=True\n",
    "):\n",
    "    print(data.choices[0].text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Switching to chat API\n",
    "chat API required for gpt-3.5-turbo/gpt-4 models \n",
    "Note: max_tokens defaults to inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je veux un frox domestique.\n"
     ]
    }
   ],
   "source": [
    "reply = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates English to French.\"},\n",
    "      {\"role\": \"user\", \"content\": 'Translate the following English text to French: I want a pet frog'}\n",
    "    ]\n",
    ")\n",
    "print(reply.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"generate me 3 trivia questions and answers\"}\n",
    "    ]\n",
    ")\n",
    "print(reply.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "reply = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that classifies the sentiment in text as either positive, neutral, or negative\"},\n",
    "        {\"role\": \"user\", \"content\": \"Classify the sentiment in the following text: 'I really hate chickens'\" },\n",
    "        {\"role\": \"assistant\", \"content\": \"Negative\" },\n",
    "        {\"role\": \"user\", \"content\": \"Classify the sentiment in the following text: 'I love my dog'\" },\n",
    "    ]\n",
    ")\n",
    "print(reply.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a world of chaos, where life's a race,\n",
      "Let's find solace in laughter's embrace.\n",
      "With humor as our guiding light,\n",
      "We'll navigate the darkest night.\n",
      "For in laughter, we'll always find our place.\n"
     ]
    }
   ],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that creates funny poems\"},\n",
    "        {\"role\": \"user\", \"content\": \"Generate me a 5 line poem about the topic of your choosing\"}\n",
    "    ],\n",
    "    temperature=0.9,\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "jupyter_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
